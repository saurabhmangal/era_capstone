{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11cb27-d931-4ac1-a1fc-3e79fa5c1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F, LayerNorm, Linear, Sequential, GELU\n",
    "from transformers import AutoTokenizer, AutoProcessor, CLIPVisionModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "from step2_dataset import llavadataset, collate_fn\n",
    "\n",
    "# Set proxy environment variables if needed\n",
    "proxy_address = 'http://185.46.212.90:80'\n",
    "for protocol in ['http', 'https']:\n",
    "    os.environ[f'{protocol}_proxy'] = proxy_address\n",
    "    os.environ[f'{protocol.upper()}_PROXY'] = proxy_address\n",
    "\n",
    "# Model names\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name = \"microsoft/phi-2\"\n",
    "\n",
    "# Load tokenizer and processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(clip_model_name)\n",
    "tokenizer.add_tokens('[QA]')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Configuration\n",
    "train_batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_workers = 10\n",
    "\n",
    "# DataLoader setup\n",
    "qa_dataset = pd.read_csv('train_token.csv')\n",
    "train_dataloader = DataLoader(llavadataset(qa_dataset, phi_model_name, clip_model_name, processor),\n",
    "                              collate_fn=collate_fn, \n",
    "                              batch_size=train_batch_size, \n",
    "                              num_workers=num_workers, \n",
    "                              shuffle=True, \n",
    "                              pin_memory=True)\n",
    "\n",
    "# Load validation data\n",
    "with open('sample_val_data.csv', newline='') as file:\n",
    "    sample_val_data = list(csv.reader(file))\n",
    "\n",
    "# Model Setup\n",
    "clip_model = CLIPVisionModel.from_pretrained(clip_model_name).to(device)\n",
    "clip_model.eval()  # Set CLIP model to evaluation mode\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "                                                phi_model_name,\n",
    "                                                torch_dtype=torch.float32,\n",
    "                                                quantization_config=BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                                                                       bnb_4bit_quant_type=\"nf4\", \n",
    "                                                                                       bnb_4bit_compute_dtype=torch.float16),\n",
    "                                                trust_remote_code=True).to(device)\n",
    "\n",
    "# Projection and ResBlock\n",
    "projection = Linear(768, 2560).to(device)\n",
    "resblock = Sequential(LayerNorm(2560), Linear(2560, 2560), GELU(), Linear(2560, 2560)).to(device)\n",
    "\n",
    "\n",
    "# PEFT Model\n",
    "peft_model = PeftModel(phi_model, \n",
    "                       LoraConfig(lora_alpha=16, \n",
    "                                  lora_dropout=0.1, \n",
    "                                  r=64, \n",
    "                                  bias=\"none\", \n",
    "                                  task_type=\"CAUSAL_LM\",\n",
    "                                  target_modules=[\"q_proj\", 'k_proj', 'v_proj', 'fc1', 'fc2']\n",
    "                                  )).to(device)\n",
    "\n",
    "# Load checkpoints if available\n",
    "for model_part, model_path in [('projection', './model_chkpt/step2_projection.pth'), ('resblock', './model_chkpt/step2_resblock.pth')]:\n",
    "    if os.path.isfile(model_path):\n",
    "        getattr(peft_model, model_part).load_state_dict(torch.load(model_path))\n",
    "        print(f\"Loaded {model_part} from checkpoint\")\n",
    "\n",
    "# Function for running validation\n",
    "# random validation prediction\n",
    "def model_run_val(sample_val_data,max_generate_length=10):\n",
    "\n",
    "    total_val_len = len(sample_val_data)\n",
    "    random_val_datapoint = random.randrange(1,total_val_len) # 0 is header\n",
    "    \n",
    "    val_image_url = sample_val_data[random_val_datapoint][0]\n",
    "    val_q = sample_val_data[random_val_datapoint][1]\n",
    "    val_a = sample_val_data[random_val_datapoint][2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_load = Image.open(requests.get(val_image_url,stream=True).raw)\n",
    "        image_processed = processor(images=image_load, return_tensors=\"pt\").to(device)\n",
    "        clip_val_outputs = clip_model(**image_processed).last_hidden_state[:,1:,:]\n",
    "        val_image_embeds = projection(clip_val_outputs)\n",
    "        val_image_embeds = resblock(val_image_embeds).to(torch.float16)\n",
    "        \n",
    "        \n",
    "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).to(device)\n",
    "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        val_q_tokenised = tokenizer(val_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "        val_q_embeds  = peft_model.model.model.embed_tokens(val_q_tokenised).unsqueeze(0)\n",
    "        \n",
    "        val_combined_embeds = torch.cat([val_image_embeds, img_token_embeds, val_q_embeds], dim=1) # 1, 69, 2560\n",
    "\n",
    "        predicted_caption = peft_model.generate(inputs_embeds=val_combined_embeds,\n",
    "                                                  max_new_tokens=max_generate_length,\n",
    "                                                  return_dict_in_generate = True)\n",
    "    \n",
    "        predicted_captions_decoded = tokenizer.batch_decode(predicted_caption.sequences[:, 1:])[0] \n",
    "        predicted_captions_decoded = predicted_captions_decoded.replace(\"<|endoftext|>\", \"\")\n",
    "\n",
    "    print(f\"Image: {val_image_url}\")\n",
    "    print(f\"Question: {val_q}\")\n",
    "    print(f\"Answer:   {val_a}\")\n",
    "    print(f\"Model Predicted Ans: {predicted_captions_decoded}\")\n",
    "    \n",
    "print(model_run_val(sample_val_data,max_generate_length=100))\n",
    "\n",
    "# Training setup\n",
    "optimizers = {\n",
    "    'phi': torch.optim.Adam(peft_model.parameters(), lr=1e-6),\n",
    "    'projection': torch.optim.Adam(projection.parameters(), lr=1e-5),\n",
    "    'resblock': torch.optim.Adam(resblock.parameters(), lr=1e-5),\n",
    "}\n",
    "\n",
    "# Training loop - Similar to the original, with necessary adjustments for efficiency and readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1aded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from step2_dataset import llavadataset, collate_fn\n",
    "import pickle\n",
    "import peft\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer,BitsAndBytesConfig, AutoModelForCausalLM, CLIPVisionModel, AutoProcessor\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "import csv\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy setup, if necessary\n",
    "try:\n",
    "    os.environ['HTTP_PROXY'] = 'http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://185.46.212.90:80'\n",
    "    print(\"Proxy exported\")\n",
    "except Exception as e:\n",
    "    print(\"Could not set proxy:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['http_proxy'] ='http://abmcpl-blr.it%40adityabirla.com:birla%402019@165.225.104.42'\n",
    "os.environ['https_proxy'] ='http://abmcpl-blr.it%40adityabirla.com:birla%402019@165.225.104.42'\n",
    "os.environ['HTTP_PROXY'] ='http://abmcpl-blr.it%40adityabirla.com:birla%402019@165.225.104.42'\n",
    "os.environ['HTTPS_PROXY']='http://abmcpl-blr.it%40adityabirla.com:birla%402019@165.225.104.42'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d568a-3fe5-4130-a504-9f8861920981",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name  = \"microsoft/phi-2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
    "tokenizer.add_tokens('[QA]')\n",
    "tokenizer.add_special_tokens({'pad_token':'[PAD]'}) \n",
    "train_batch_size    = 32\n",
    "clip_embed = 768\n",
    "phi_embed  = 2560\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers = 10\n",
    "IMAGE_TOKEN_ID = 23893 # token for word comment\n",
    "max_steps      = 100000\n",
    "EOS_TOKEN_ID   = 50256\n",
    "phi_patches    = 49\n",
    "vocab_size     = 51200\n",
    "max_generate_length = 100\n",
    "model_val_step      = 1000\n",
    "model_log_step      = 100\n",
    "model_save_step     = 100\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer('[QA]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692e0ef-165c-40c5-a6a0-a601d1c5099c",
   "metadata": {},
   "source": [
    "# 1 - DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844cdb1-2a30-4d65-a129-606d54e2b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "csv_file = 'train_token.csv'\n",
    "qa_dataset = pd.read_csv(csv_file)\n",
    "\n",
    "# data loaders\n",
    "train_dataloader = DataLoader(llavadataset(qa_dataset, phi_model_name,clip_model_name,processor),\n",
    "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = num_workers, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bccc4c4-55d7-46db-aba1-5871bf88c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('sample_val_data.csv')\n",
    "csvreader = csv.reader(file)\n",
    "sample_val_data = []\n",
    "for row in csvreader:\n",
    "    sample_val_data.append(row)\n",
    "print(sample_val_data[1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f57a66-8827-47b9-b006-d409d6ce4cc2",
   "metadata": {},
   "source": [
    "# 2 - MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a977832a-7109-4125-9570-388496c0e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResBlock(nn.Module):\n",
    "    def __init__(self, phi_embed):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.LayerNorm(phi_embed)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(phi_embed, phi_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(phi_embed, phi_embed)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.pre_norm(x)\n",
    "        return x + self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46a021-3811-4297-9604-b4d4b181cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPVisionModel.from_pretrained(clip_model_name).to(device)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,)\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi_model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "phi_model.config.use_cache = False\n",
    "projection = torch.nn.Linear(clip_embed, phi_embed).to(device)\n",
    "resblock = SimpleResBlock(phi_embed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e720ab-4f63-487d-ad9b-97ab0dd2300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'fc1',\n",
    "        'fc2'\n",
    "    ]\n",
    ")\n",
    "peft_model = peft.get_peft_model(phi_model, peft_config).to(device)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219df78-1e2b-4730-81d6-5a7f794d8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip non trainable\n",
    "for network in [clip_model]:\n",
    "    for param in network.parameters():\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885fcad-2060-42c0-b2e9-e6b9469fde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check trainable paramaeters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"PEFT MODEL:{count_parameters(peft_model)}\")\n",
    "print(f\"PROJECTION MODEL:{count_parameters(projection)}\")\n",
    "print(f\"CLIP MODEL:{count_parameters(clip_model)}\")\n",
    "print(f\"PHI MODEL:{count_parameters(phi_model)}\")\n",
    "print(f\"RESNET MODEL:{count_parameters(resblock)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7fd86-4d63-41f6-a23e-81501fa08ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('model_chkpt/step2_projection.pth'):\n",
    "    projection.load_state_dict(torch.load('./model_chkpt/step2_projection.pth'))\n",
    "    resblock.load_state_dict(torch.load('./model_chkpt/step2_resblock.pth'))\n",
    "    peft_model.from_pretrained(phi_model,'./model_chkpt/lora_adaptor')\n",
    "    print(\"Loaded step2 checkpoint\")\n",
    "\n",
    "else:\n",
    "    projection.load_state_dict(torch.load('./model_chkpt/step1_projection.pth'))\n",
    "    resblock.load_state_dict(torch.load('./model_chkpt/step1_resblock.pth'))\n",
    "    print(\"Loaded step1 checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745aeb44-44fe-4579-8d84-29e33e8a660f",
   "metadata": {},
   "source": [
    "# 3 - FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950cd90-fb80-4991-9b9d-5855bded45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random validation prediction\n",
    "def model_run_val(sample_val_data,max_generate_length=10):\n",
    "\n",
    "    total_val_len = len(sample_val_data)\n",
    "    random_val_datapoint = random.randrange(1,total_val_len) # 0 is header\n",
    "    \n",
    "    val_image_url = sample_val_data[random_val_datapoint][0]\n",
    "    val_q = sample_val_data[random_val_datapoint][1]\n",
    "    val_a = sample_val_data[random_val_datapoint][2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_load = Image.open(requests.get(val_image_url,stream=True).raw)\n",
    "        image_processed = processor(images=image_load, return_tensors=\"pt\").to(device)\n",
    "        clip_val_outputs = clip_model(**image_processed).last_hidden_state[:,1:,:]\n",
    "        val_image_embeds = projection(clip_val_outputs)\n",
    "        val_image_embeds = resblock(val_image_embeds).to(torch.float16)\n",
    "        \n",
    "        \n",
    "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).to(device)\n",
    "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        val_q_tokenised = tokenizer(val_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "        val_q_embeds  = peft_model.model.model.embed_tokens(val_q_tokenised).unsqueeze(0)\n",
    "        \n",
    "        val_combined_embeds = torch.cat([val_image_embeds, img_token_embeds, val_q_embeds], dim=1) # 1, 69, 2560\n",
    "\n",
    "        predicted_caption = peft_model.generate(inputs_embeds=val_combined_embeds,\n",
    "                                                  max_new_tokens=max_generate_length,\n",
    "                                                  return_dict_in_generate = True)\n",
    "    \n",
    "        predicted_captions_decoded = tokenizer.batch_decode(predicted_caption.sequences[:, 1:])[0] \n",
    "        predicted_captions_decoded = predicted_captions_decoded.replace(\"<|endoftext|>\", \"\")\n",
    "\n",
    "    print(f\"Image: {val_image_url}\")\n",
    "    print(f\"Question: {val_q}\")\n",
    "    print(f\"Answer:   {val_a}\")\n",
    "    print(f\"Model Predicted Ans: {predicted_captions_decoded}\")\n",
    "    \n",
    "model_run_val(sample_val_data,max_generate_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896295f-05f9-4541-bfdd-ea41de8cbb14",
   "metadata": {},
   "source": [
    "# 3 - TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d0019-0cb7-4a6a-bc6e-bf7033374bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_optimizer        = torch.optim.Adam(peft_model.parameters(), lr=1e-6)\n",
    "projection_optimizer = torch.optim.Adam(projection.parameters(), lr=1e-5)\n",
    "resnet_optimizer     = torch.optim.Adam(resblock.parameters(),   lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e97f3-ff51-4b5c-9bcf-ef8c9ec4c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "running_loss = 0.\n",
    "projection.train()\n",
    "peft_model.train()\n",
    "resblock.train()\n",
    "\n",
    "for epoch in range(1000000):\n",
    "    for batch_idx, (images,questions,answers) in enumerate(train_dataloader):\n",
    "\n",
    "        # process input data\n",
    "        batch_size = questions.size(0)\n",
    "        questions  = questions.to(device)\n",
    "        answers    = answers.to(device)\n",
    "\n",
    "        # clip\n",
    "        images = {'pixel_values': images.to(device)}\n",
    "        clip_outputs  = clip_model(**images)\n",
    "        images_embeds = clip_outputs.last_hidden_state[:,1:,:] # remove cls token\n",
    "        \n",
    "        # projection\n",
    "        image_embeds  = projection(images_embeds)\n",
    "        image_embeds  = resblock(image_embeds).to(torch.float16)\n",
    "\n",
    "        # embeds\n",
    "        #print(questions.shape,answers.shape)\n",
    "        img_token_tensor = torch.tensor(IMAGE_TOKEN_ID).repeat(batch_size, 1).to(device)\n",
    "        img_token_embeds = peft_model.model.model.embed_tokens(img_token_tensor)\n",
    "        questions_embed  = peft_model.model.model.embed_tokens(questions)\n",
    "\n",
    "        # forward pass\n",
    "        #print(\"***************\")\n",
    "        combined_embeds = torch.cat([image_embeds, img_token_embeds, questions_embed], dim=1) # 4, 69, 2560\n",
    "        #print(f\"combined_embeds shape{combined_embeds.shape}\")\n",
    "        phi_output_logits = peft_model(inputs_embeds=combined_embeds)['logits'] # 4, 69, 51200\n",
    "        #print(f\"phi_output_logits shape{phi_output_logits.shape}\")\n",
    "        #print(f\"answers shape {answers.shape}\")\n",
    "\n",
    "        # take out the image embeddings\n",
    "        phi_output_logits = phi_output_logits[:,images_embeds.shape[1] + 1 : ,:]\n",
    "        #print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
    "        phi_output_logits = phi_output_logits.reshape(-1,vocab_size)\n",
    "        #print(f\"phi_output_logits after shape{phi_output_logits.shape}\")\n",
    "        #print(f\"answers after shape {answers.contiguous().view(-1).shape}\")\n",
    "\n",
    "        phi_optimizer.zero_grad()\n",
    "        projection_optimizer.zero_grad()\n",
    "        resnet_optimizer.zero_grad()\n",
    "        \n",
    "        loss = F.cross_entropy(phi_output_logits, answers.contiguous().view(-1), ignore_index=50296,label_smoothing=0.1)\n",
    "\n",
    "        # loss backprop\n",
    "        loss.backward()\n",
    "        phi_optimizer.step()\n",
    "        projection_optimizer.step()\n",
    "        resnet_optimizer.step()\n",
    "        \n",
    "\n",
    "        if step % model_log_step == 0:\n",
    "            print(f\"Iteration {step}/{max_steps}, Loss: {loss.item()}\")\n",
    "\n",
    "        if step % model_val_step == 0:\n",
    "            projection.eval()\n",
    "            peft_model.eval()\n",
    "            resblock.eval()\n",
    "            model_run_val(sample_val_data,max_generate_length)\n",
    "            projection.train()\n",
    "            peft_model.train()\n",
    "            resblock.train()\n",
    "\n",
    "        if step % model_save_step == 0:\n",
    "            print(\"Saving Checkpoint\")\n",
    "            torch.save(projection.state_dict(),'./model_chkpt/step2_projection.pth')\n",
    "            torch.save(resblock.state_dict(),'./model_chkpt/step2_resblock.pth')\n",
    "            peft_model.save_pretrained('./model_chkpt/lora_adaptor/', save_adapter=True, save_config=True)\n",
    "            \n",
    "        if step >= max_steps:\n",
    "            print(\"Training finished.\")\n",
    "            break\n",
    "            \n",
    "        print({\"step\": step, \"train_loss\": loss.item()})\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d4b60-d49a-484a-8c87-cd7bf356cbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
