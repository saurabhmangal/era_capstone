{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy exported\n"
     ]
    }
   ],
   "source": [
    "# Proxy setup, if necessary\n",
    "try:\n",
    "    os.environ['HTTP_PROXY'] = 'http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://185.46.212.90:80'\n",
    "    print(\"Proxy exported\")\n",
    "except Exception as e:\n",
    "    print(\"Could not set proxy:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['http_proxy'] ='http://abmcpl-blr.it%40adityabirla.com:birla%402019@165.225.104.42'\n",
    "os.environ['https_proxy'] ='http://abmcpl-blr.it%40adityabirla.com:birla%402019@165.225.104.42'\n",
    "os.environ['HTTP_PROXY'] ='http://abmcpl-blr.it%40adityabirla.com:birla%402019@165.225.104.42'\n",
    "os.environ['HTTPS_PROXY']='http://abmcpl-blr.it%40adityabirla.com:birla%402019@165.225.104.42'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - GET INSTRUCT150K DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCz4GF4k94du",
    "outputId": "fb8e5342-8c7f-4c3b-92f1-67b22880008b"
   },
   "outputs": [],
   "source": [
    "! wget https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5zCY4DO95Zm",
    "outputId": "34b312e4-83e0-450a-d80b-0720123dafa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '000000033471',\n",
       " 'image': '000000033471.jpg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nWhat are the colors of the bus in the image?'},\n",
       "  {'from': 'gpt', 'value': 'The bus in the image is white and red.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What feature can be seen on the back of the bus?'},\n",
       "  {'from': 'gpt', 'value': 'The back of the bus features an advertisement.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Is the bus driving down the street or pulled off to the side?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The bus is driving down the street, which is crowded with people and other vehicles.'}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening JSON file - instruct150k\n",
    "f = open('llava_instruct_150k.json')\n",
    "\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - ASSEMBLE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWoVMp2C-D8b",
    "outputId": "2f04aeb0-8fa4-4361-9a9e-3638f1c8f899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 processed\n",
      "10000 processed\n",
      "20000 processed\n",
      "30000 processed\n",
      "40000 processed\n",
      "50000 processed\n",
      "60000 processed\n",
      "70000 processed\n",
      "80000 processed\n",
      "90000 processed\n",
      "100000 processed\n",
      "110000 processed\n",
      "120000 processed\n",
      "130000 processed\n",
      "140000 processed\n",
      "150000 processed\n"
     ]
    }
   ],
   "source": [
    "# create input pickle file by flattening the data\n",
    "data_instruct150_flatten = []\n",
    "r = 0\n",
    "\n",
    "for a_idx,d in enumerate(data):\n",
    "    image = d['image']\n",
    "    image_url = 'http://images.cocodataset.org/train2017/' + image\n",
    "    conv_iter = iter( d['conversations'])\n",
    "    for i in conv_iter:\n",
    "      gpt_ans = next(conv_iter)\n",
    "      if len(gpt_ans['value']) > 200: # filter out too long answers\n",
    "          continue\n",
    "      if i['from'] == 'human' and gpt_ans['from'] == 'gpt':\n",
    "        data_instruct150_flatten.append((image_url, i['value'].replace('<image>\\n','').replace('\\n<image>',''),gpt_ans['value']))\n",
    "\n",
    "    if a_idx % 10000 == 0:\n",
    "      print(f\"{10000 * r} processed\")\n",
    "      r += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://images.cocodataset.org/train2017/000000033471.jpg',\n",
       " 'What feature can be seen on the back of the bus?',\n",
       " 'The back of the bus features an advertisement.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_instruct150_flatten[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('[PAD]', '<|endoftext|>')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add tokens\n",
    "phi_model_name  = \"microsoft/phi-2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "tokenizer.add_tokens('[QA]')\n",
    "tokenizer.add_special_tokens({'pad_token':'[PAD]'}) \n",
    "tokenizer.pad_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk after 100000 rows\n",
      "Writing to disk after 200000 rows\n",
      "Writing to disk after 300000 rows\n",
      "Writing to disk after 400000 rows\n",
      "Writing to disk after 500000 rows\n",
      "Writing to disk after 600000 rows\n",
      "Writing to disk after 700000 rows\n",
      "Writing to disk after 800000 rows\n",
      "Writing to disk after 900000 rows\n",
      "Writing to disk after 1000000 rows\n",
      "Writing to disk after 1100000 rows\n",
      "Writing to disk after 1200000 rows\n",
      "Writing to disk after 1300000 rows\n",
      "Writing to disk after 1400000 rows\n",
      "Writing to disk after 1500000 rows\n",
      "Writing to disk after 1600000 rows\n",
      "Writing to disk after 1700000 rows\n",
      "Writing to disk after 1800000 rows\n",
      "Writing to disk after 1900000 rows\n",
      "Writing to disk after 2000000 rows\n",
      "Writing to disk after 2100000 rows\n",
      "Writing to disk after 2200000 rows\n",
      "Writing to disk after 2300000 rows\n",
      "Writing to disk after 2400000 rows\n",
      "Writing to disk after 2500000 rows\n",
      "Writing to disk after 2600000 rows\n",
      "Writing to disk after 2700000 rows\n",
      "Writing to disk after 2800000 rows\n",
      "Writing to disk after 2900000 rows\n",
      "Writing to disk after 3000000 rows\n",
      "Writing to disk after 3100000 rows\n",
      "Writing to disk after 3200000 rows\n",
      "Writing to disk after 3300000 rows\n",
      "Writing to disk after 3400000 rows\n",
      "Writing to disk after 3500000 rows\n",
      "Writing to disk after 3600000 rows\n",
      "Writing to disk after 3700000 rows\n",
      "Writing to disk after 3800000 rows\n",
      "Writing to disk after 3900000 rows\n"
     ]
    }
   ],
   "source": [
    "# gpt like training dataset\n",
    "with open('train_token.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([['img_url','input','label']])\n",
    "    \n",
    "train_data_temp = []\n",
    "r = 1\n",
    "for df in data_instruct150_flatten:\n",
    "  image_url = df[0]\n",
    "  image_q   = df[1] + ' [QA]'\n",
    "  image_a   = df[2] +  tokenizer.eos_token\n",
    "  #print(image_q)\n",
    "  #print(image_a)\n",
    "  \n",
    "  # tokenise \n",
    "  ques_token = tokenizer(image_q, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "  ans_token  = tokenizer(image_a, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "\n",
    "  #print(ques_token)\n",
    "  #print(ans_token)\n",
    "  #break\n",
    "\n",
    "  context_length = len(ques_token)\n",
    "  combo_q_a = torch.cat( [ques_token,ans_token])\n",
    "    \n",
    "  for al in range(len(ans_token)):   \n",
    "    input = combo_q_a[al : al + context_length].numpy()\n",
    "    label = combo_q_a[al + 1 : al + context_length + 1].numpy()\n",
    "    train_data_temp.append([image_url,input,label])\n",
    "    if len(train_data_temp) >= 100000: # write to the file\n",
    "       print(f\"Writing to disk after {r * 100000} rows\")\n",
    "       r += 1\n",
    "       with open('train_token.csv', 'a', newline='') as file:\n",
    "          writer = csv.writer(file)\n",
    "          writer.writerows(train_data_temp)\n",
    "       train_data_temp = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOP-GNDK40Ff"
   },
   "source": [
    "# 3 - PYTORCH DATASET AND DATALOADER TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dMQZedDH4vVa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_url</th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 2061   389   262  7577   286   262  1323   2...</td>\n",
       "      <td>[  389   262  7577   286   262  1323   287   2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[  389   262  7577   286   262  1323   287   2...</td>\n",
       "      <td>[  262  7577   286   262  1323   287   262  29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[  262  7577   286   262  1323   287   262  29...</td>\n",
       "      <td>[ 7577   286   262  1323   287   262  2939    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 7577   286   262  1323   287   262  2939    ...</td>\n",
       "      <td>[  286   262  1323   287   262  2939    30   2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[  286   262  1323   287   262  2939    30   2...</td>\n",
       "      <td>[  262  1323   287   262  2939    30   220 502...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[  262  1323   287   262  2939    30   220 502...</td>\n",
       "      <td>[ 1323   287   262  2939    30   220 50295   4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 1323   287   262  2939    30   220 50295   4...</td>\n",
       "      <td>[  287   262  2939    30   220 50295   464  13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[  287   262  2939    30   220 50295   464  13...</td>\n",
       "      <td>[  262  2939    30   220 50295   464  1323   2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[  262  2939    30   220 50295   464  1323   2...</td>\n",
       "      <td>[ 2939    30   220 50295   464  1323   287   2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>[ 2939    30   220 50295   464  1323   287   2...</td>\n",
       "      <td>[   30   220 50295   464  1323   287   262  29...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             img_url  \\\n",
       "0  http://images.cocodataset.org/train2017/000000...   \n",
       "1  http://images.cocodataset.org/train2017/000000...   \n",
       "2  http://images.cocodataset.org/train2017/000000...   \n",
       "3  http://images.cocodataset.org/train2017/000000...   \n",
       "4  http://images.cocodataset.org/train2017/000000...   \n",
       "5  http://images.cocodataset.org/train2017/000000...   \n",
       "6  http://images.cocodataset.org/train2017/000000...   \n",
       "7  http://images.cocodataset.org/train2017/000000...   \n",
       "8  http://images.cocodataset.org/train2017/000000...   \n",
       "9  http://images.cocodataset.org/train2017/000000...   \n",
       "\n",
       "                                               input  \\\n",
       "0  [ 2061   389   262  7577   286   262  1323   2...   \n",
       "1  [  389   262  7577   286   262  1323   287   2...   \n",
       "2  [  262  7577   286   262  1323   287   262  29...   \n",
       "3  [ 7577   286   262  1323   287   262  2939    ...   \n",
       "4  [  286   262  1323   287   262  2939    30   2...   \n",
       "5  [  262  1323   287   262  2939    30   220 502...   \n",
       "6  [ 1323   287   262  2939    30   220 50295   4...   \n",
       "7  [  287   262  2939    30   220 50295   464  13...   \n",
       "8  [  262  2939    30   220 50295   464  1323   2...   \n",
       "9  [ 2939    30   220 50295   464  1323   287   2...   \n",
       "\n",
       "                                               label  \n",
       "0  [  389   262  7577   286   262  1323   287   2...  \n",
       "1  [  262  7577   286   262  1323   287   262  29...  \n",
       "2  [ 7577   286   262  1323   287   262  2939    ...  \n",
       "3  [  286   262  1323   287   262  2939    30   2...  \n",
       "4  [  262  1323   287   262  2939    30   220 502...  \n",
       "5  [ 1323   287   262  2939    30   220 50295   4...  \n",
       "6  [  287   262  2939    30   220 50295   464  13...  \n",
       "7  [  262  2939    30   220 50295   464  1323   2...  \n",
       "8  [ 2939    30   220 50295   464  1323   287   2...  \n",
       "9  [   30   220 50295   464  1323   287   262  29...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('train_token.csv')\n",
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2061   389   262  7577   286   262  1323   287   262  2939    30   220\n",
      " 50295]\n",
      "[  389   262  7577   286   262  1323   287   262  2939    30   220 50295\n",
      "   464]\n"
     ]
    }
   ],
   "source": [
    "for i in (df_data[0:1]['input']):\n",
    "    print(i)\n",
    "for i in (df_data[0:1]['label']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "iSQeuVRR4_W7"
   },
   "outputs": [],
   "source": [
    "class llavadataset(Dataset):\n",
    "  def __init__(self, qa_dataset, phi_model_name, clip_model_name, tokenizer):\n",
    "    self.processor  = AutoProcessor.from_pretrained(clip_model_name)\n",
    "    self.qa_dataset = qa_dataset\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.qa_dataset.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # from image perspective\n",
    "    img_url = self.qa_dataset.img_url[idx]\n",
    "    ques    = torch.tensor(np.array(np.matrix(self.qa_dataset.input[idx]))[0])  \n",
    "    ans     = torch.tensor(np.array(np.matrix(self.qa_dataset.label[idx]))[0])\n",
    "    \n",
    "    # image load\n",
    "    image_load = Image.open(requests.get(img_url,stream=True).raw)\n",
    "    image_processed = self.processor(images=image_load, return_tensors=\"pt\") ['pixel_values']\n",
    "    image_processed = image_processed.squeeze(0)\n",
    "    # q = self.tokenizer(ques, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "    # a = self.tokenizer(ans, return_tensors=\"pt\", return_attention_mask=False)['input_ids'].squeeze(0)\n",
    "    return(image_processed , ques, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Phk2K52q5wy1",
    "outputId": "4ed8cebb-2b0a-4b0c-a32b-11ae53031835"
   },
   "outputs": [],
   "source": [
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "#phi_model_name  = \"microsoft/phi-2\"\n",
    "#tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
    "csv_file = 'train_token.csv'\n",
    "qa_dataset = pd.read_csv(csv_file)\n",
    "step2_dataset = llavadataset(qa_dataset, phi_model_name, clip_model_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JarVPVPe6IwR",
    "outputId": "ffed174a-8ce7-4d1a-ac1a-7afba1df9316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.1858,  1.1566,  1.1274,  ...,  1.3902,  1.3756,  1.1712],\n",
       "          [ 1.2588,  1.2296,  1.1858,  ...,  1.4048,  1.2150,  1.2734],\n",
       "          [ 1.4194,  1.3318,  1.2880,  ...,  1.3610,  1.1128,  1.3756],\n",
       "          ...,\n",
       "          [-0.4346, -0.3762, -0.3762,  ...,  0.8209,  0.7041,  0.9522],\n",
       "          [-0.4200, -0.4054, -0.3908,  ...,  0.1493,  0.1639,  0.1055],\n",
       "          [-0.4346, -0.4346, -0.3908,  ...,  0.2223,  0.1493,  0.1347]],\n",
       " \n",
       "         [[ 1.4145,  1.3695,  1.3395,  ...,  1.6397,  1.6247,  1.3995],\n",
       "          [ 1.4596,  1.4446,  1.3995,  ...,  1.6397,  1.4446,  1.5046],\n",
       "          [ 1.5646,  1.4896,  1.4295,  ...,  1.5946,  1.3395,  1.6096],\n",
       "          ...,\n",
       "          [-0.4014, -0.3264, -0.3264,  ...,  0.9793,  0.7542,  1.0393],\n",
       "          [-0.3564, -0.3414, -0.3264,  ...,  0.2439,  0.2289,  0.1839],\n",
       "          [-0.3714, -0.3714, -0.3264,  ...,  0.2439,  0.1989,  0.1839]],\n",
       " \n",
       "         [[ 1.7904,  1.7904,  1.7477,  ...,  1.9895,  1.9610,  1.7477],\n",
       "          [ 1.8473,  1.8331,  1.7904,  ...,  1.9895,  1.8046,  1.8473],\n",
       "          [ 1.9326,  1.8899,  1.8331,  ...,  1.9468,  1.7051,  1.9610],\n",
       "          ...,\n",
       "          [-0.1009, -0.0440, -0.0440,  ...,  1.1647,  0.8661,  1.1932],\n",
       "          [-0.0867, -0.0582, -0.0440,  ...,  0.3826,  0.4395,  0.3399],\n",
       "          [-0.1151, -0.1009, -0.0440,  ...,  0.4679,  0.4110,  0.3826]]]),\n",
       " tensor([ 2061,   389,   262,  7577,   286,   262,  1323,   287,   262,  2939,\n",
       "            30,   220, 50295]),\n",
       " tensor([  389,   262,  7577,   286,   262,  1323,   287,   262,  2939,    30,\n",
       "           220, 50295,   464]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PG93q8ZR_rlg"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    image_embeddings, ques, ans = zip(*batch)\n",
    "    image_embeddings_stacked = torch.stack(image_embeddings, dim=0)\n",
    "    ques_padded = torch.nn.utils.rnn.pad_sequence(ques, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    ans_padded = torch.nn.utils.rnn.pad_sequence(ans, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return (image_embeddings_stacked, ques_padded,ans_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbZ6KsB3BjMR",
    "outputId": "856a5ef9-9719-4499-c76b-12f5c52c233b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50296, 50256)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id,tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ds3UxzU_eTA",
    "outputId": "0f5d1815-68a3-41a0-8225-8d14a47473fb"
   },
   "outputs": [],
   "source": [
    "val_dataloader   = DataLoader(llavadataset(qa_dataset, phi_model_name,clip_model_name,tokenizer),\n",
    "                      collate_fn=collate_fn, batch_size=2, num_workers = 10, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txeV8H8c_267",
    "outputId": "8b5ec949-a056-431b-ac36-699c3f8129d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-0.6682, -0.6682, -0.6244,  ..., -0.3178, -0.3178, -0.3178],\n",
       "           [-0.6682, -0.6682, -0.6390,  ..., -0.3178, -0.3032, -0.3032],\n",
       "           [-0.5660, -0.5368, -0.5660,  ..., -0.3178, -0.3032, -0.2886],\n",
       "           ...,\n",
       "           [-1.5003, -1.5441, -1.5441,  ..., -0.1426, -0.1572, -0.1572],\n",
       "           [-1.2813, -1.3105, -1.2521,  ..., -0.1572, -0.1572, -0.1718],\n",
       "           [-1.2813, -1.2813, -1.2667,  ..., -0.1572, -0.1280, -0.1718]],\n",
       " \n",
       "          [[-0.6415, -0.6115, -0.6265,  ..., -0.3264, -0.3564, -0.3564],\n",
       "           [-0.6115, -0.5965, -0.5965,  ..., -0.3264, -0.3264, -0.3264],\n",
       "           [-0.5815, -0.5665, -0.5665,  ..., -0.3264, -0.3264, -0.3114],\n",
       "           ...,\n",
       "           [-1.5270, -1.5120, -1.5120,  ..., -0.3414, -0.3564, -0.3564],\n",
       "           [-1.3319, -1.3019, -1.2418,  ..., -0.3564, -0.3564, -0.3564],\n",
       "           [-1.3469, -1.3319, -1.3019,  ..., -0.3564, -0.3264, -0.3564]],\n",
       " \n",
       "          [[-0.6981, -0.6839, -0.6555,  ..., -0.5417, -0.5559, -0.5275],\n",
       "           [-0.6697, -0.6697, -0.6697,  ..., -0.5133, -0.5275, -0.5417],\n",
       "           [-0.6412, -0.6270, -0.6270,  ..., -0.4848, -0.4848, -0.5275],\n",
       "           ...,\n",
       "           [-1.3807, -1.3665, -1.3665,  ..., -0.5986, -0.6270, -0.6270],\n",
       "           [-1.2669, -1.2811, -1.2385,  ..., -0.6128, -0.6270, -0.6270],\n",
       "           [-1.1958, -1.2243, -1.2243,  ..., -0.6128, -0.5986, -0.6270]]],\n",
       " \n",
       " \n",
       "         [[[-0.8726, -0.8726, -0.8726,  ..., -0.1572, -0.1572, -0.1718],\n",
       "           [-0.8580, -0.8580, -0.8434,  ..., -0.1426, -0.1426, -0.1718],\n",
       "           [-0.8580, -0.8726, -0.8580,  ..., -0.1426, -0.1572, -0.1718],\n",
       "           ...,\n",
       "           [-1.5149, -1.4711, -1.4711,  ...,  0.6895,  0.6895,  0.6603],\n",
       "           [-1.4711, -1.4419, -1.4565,  ...,  0.6019,  0.5873,  0.6019],\n",
       "           [-1.5003, -1.4711, -1.4711,  ...,  0.6311,  0.5727,  0.6019]],\n",
       " \n",
       "          [[-0.2063, -0.1913, -0.1913,  ...,  0.6191,  0.6191,  0.6341],\n",
       "           [-0.1763, -0.1763, -0.1613,  ...,  0.6341,  0.6341,  0.6341],\n",
       "           [-0.1763, -0.1763, -0.1613,  ...,  0.6341,  0.6191,  0.6341],\n",
       "           ...,\n",
       "           [-1.4069, -1.3919, -1.4069,  ...,  0.7542,  0.7242,  0.7242],\n",
       "           [-1.3919, -1.4069, -1.4219,  ...,  0.6942,  0.6792,  0.6942],\n",
       "           [-1.4219, -1.4069, -1.4219,  ...,  0.6942,  0.6792,  0.6642]],\n",
       " \n",
       "          [[ 1.1505,  1.1647,  1.1647,  ...,  1.8615,  1.8615,  1.8615],\n",
       "           [ 1.1647,  1.1789,  1.1932,  ...,  1.8757,  1.8757,  1.8615],\n",
       "           [ 1.1789,  1.1789,  1.1932,  ...,  1.8615,  1.8473,  1.8473],\n",
       "           ...,\n",
       "           [-1.0110, -1.0110, -1.0394,  ...,  0.8234,  0.7950,  0.7666],\n",
       "           [-0.9825, -1.0110, -1.0394,  ...,  0.7523,  0.7523,  0.7523],\n",
       "           [-1.0110, -1.0252, -1.0536,  ...,  0.7239,  0.7381,  0.7523]]]]),\n",
       " tensor([[ 1978,   287,   257, 29556,    11,  1884, 45879,   393,  9087,  3371,\n",
       "            511, 50296],\n",
       "         [    8, 18207,   465, 22647,  3526,  4678,   379,   257, 22647,  3952,\n",
       "             13,   679]]),\n",
       " tensor([[  287,   257, 29556,    11,  1884, 45879,   393,  9087,  3371,   511,\n",
       "            686, 50296],\n",
       "         [18207,   465, 22647,  3526,  4678,   379,   257, 22647,  3952,    13,\n",
       "            679,   318]])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - SAMPLE VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\n",
    "# Opening JSON file - instruct150k\n",
    "f = open('llava_instruct_150k.json')\n",
    "\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 processed\n",
      "10000 processed\n"
     ]
    }
   ],
   "source": [
    "# create input pickle file by flattening the data\n",
    "data_instruct150_sample_val_flatten = []\n",
    "r = 0\n",
    "\n",
    "for a_idx,d in enumerate(data):\n",
    "    image = d['image']\n",
    "    image_url = 'http://images.cocodataset.org/train2017/' + image\n",
    "    conv_iter = iter( d['conversations'])\n",
    "    for i in conv_iter:\n",
    "      gpt_ans = next(conv_iter)\n",
    "      if len(gpt_ans['value']) > 200: # filter out too long answers\n",
    "          continue\n",
    "      if i['from'] == 'human' and gpt_ans['from'] == 'gpt':\n",
    "        image_q   = i['value'].replace('<image>\\n','').replace('\\n<image>','') + ' [QA]'\n",
    "        image_a   = gpt_ans['value'] +  tokenizer.eos_token\n",
    "        data_instruct150_sample_val_flatten.append([image_url, image_q, image_a ])\n",
    "\n",
    "    if a_idx % 10000 == 0:\n",
    "      print(f\"{10000 * r} processed\")\n",
    "      r += 1\n",
    "      if r >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://images.cocodataset.org/train2017/000000052846.jpg',\n",
       " 'Where is the cat positioned in the image? [QA]',\n",
       " 'The cat is positioned on top of the back of the couch in the living room.<|endoftext|>']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_instruct150_sample_val_flatten[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header\n",
    "with open('sample_val_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([['img_url','q','a']])\n",
    "\n",
    "# data\n",
    "with open('sample_val_data.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data_instruct150_sample_val_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
