{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196cb1c8-72e3-4651-9a7a-5cd8b70bcdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Proxy exported\n",
      "Using CUDA: 4 GPUs available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f6e27d36554fd9bd6a083b3d580c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 532577 and validation size 59176\n",
      "Train size 532577 and validation size 59176\n",
      "Training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/100000: Avg Running Loss = 5.348811149597168\n",
      "Step 100/100000: Avg Running Loss = 5.33362282037735\n",
      "Step 200/100000: Avg Running Loss = 5.241844072341919\n",
      "Step 300/100000: Avg Running Loss = 5.254102308750152\n",
      "Step 400/100000: Avg Running Loss = 5.234310839176178\n",
      "Step 500/100000: Avg Running Loss = 5.187703197002411\n",
      "Batch skipped as captions too long.\n",
      "Step 600/100000: Avg Running Loss = 5.267480807304382\n",
      "Batch skipped as captions too long.\n",
      "Step 700/100000: Avg Running Loss = 5.271900467872619\n",
      "Step 800/100000: Avg Running Loss = 5.24034351348877\n",
      "Step 900/100000: Avg Running Loss = 5.3043603372573855\n",
      "Saving Checkpoint\n",
      "0 - Target captions:\n",
      " A close shot of a glass container with a bundle of roses inside.  \n",
      "0 - predicted_captions:\n",
      " A plate of food on a plate a a a a a. a.....<|endoftext|> \n",
      "1 - Target captions:\n",
      " Round toddler Bento lunch boxes with character utensils<|endoftext|><|endoftext|><|endoftext|>  \n",
      "1 - predicted_captions:\n",
      " A bowl of fruit and a a of a a of a a of a a of a of<|endoftext|> \n",
      "2 - Target captions:\n",
      " A group of people are participting in a buffet.<|endoftext|><|endoftext|>  \n",
      "2 - predicted_captions:\n",
      " A man is eating a a a of a a of a a of a a of a a<|endoftext|> \n",
      "3 - Target captions:\n",
      " A computer desk topped a laptop computer and a desktop monitor.<|endoftext|><|endoftext|>  \n",
      "3 - predicted_captions:\n",
      " A computer with a keyboard and a mouse on it a. a......<|endoftext|> \n",
      "Step 1000/100000: Avg Running Loss = 5.287240035533905\n",
      "Step 1100/100000: Avg Running Loss = 5.2186582660675045\n",
      "Step 1200/100000: Avg Running Loss = 5.197671272754669\n",
      "Step 1300/100000: Avg Running Loss = 5.2177494168281555\n",
      "Step 1400/100000: Avg Running Loss = 5.181154947280884\n",
      "Step 1500/100000: Avg Running Loss = 5.175324764251709\n",
      "Step 1600/100000: Avg Running Loss = 5.228438737392426\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "Caught ProxyError in DataLoader worker process 15.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 449, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 444, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/usr/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/usr/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/usr/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='185.46.212.90', port=80): Max retries exceeded with url: http://images.cocodataset.org/train2017/000000247487.jpg (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/saurabh/era_saurabh/ERA/capstone_part2/step1_pretrain/step1_dataset.py\", line 41, in __getitem__\n    image_load = Image.open(requests.get(img_url,stream=True).raw)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/api.py\", line 73, in get\n    return request(\"get\", url, params=params, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/api.py\", line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/adapters.py\", line 513, in send\n    raise ProxyError(e, request=request)\nrequests.exceptions.ProxyError: HTTPConnectionPool(host='185.46.212.90', port=80): Max retries exceeded with url: http://images.cocodataset.org/train2017/000000247487.jpg (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_float32_matmul_precision(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMModalGPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_val_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_token_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/saurabh/era_saurabh/ERA/capstone_part2/step1_pretrain/step1_network.py:216\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_dataloader, optimizer, device, max_steps, model_save_step, model_val_step, log_step, max_token_filter, tokenizer)\u001b[0m\n\u001b[1;32m    213\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100000\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, target_captions) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m         \u001b[38;5;66;03m# manage OOM issue, skip batch for long captions\u001b[39;00m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m target_captions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_token_filter:\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch skipped as captions too long.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1326\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1325\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mProxyError\u001b[0m: Caught ProxyError in DataLoader worker process 15.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 449, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 444, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/usr/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/usr/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/usr/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/usr/local/lib/python3.8/dist-packages/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='185.46.212.90', port=80): Max retries exceeded with url: http://images.cocodataset.org/train2017/000000247487.jpg (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/saurabh/era_saurabh/ERA/capstone_part2/step1_pretrain/step1_dataset.py\", line 41, in __getitem__\n    image_load = Image.open(requests.get(img_url,stream=True).raw)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/api.py\", line 73, in get\n    return request(\"get\", url, params=params, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/api.py\", line 59, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/requests/adapters.py\", line 513, in send\n    raise ProxyError(e, request=request)\nrequests.exceptions.ProxyError: HTTPConnectionPool(host='185.46.212.90', port=80): Max retries exceeded with url: http://images.cocodataset.org/train2017/000000247487.jpg (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from model_network import CLIPPhi2Model, train_model\n",
    "from dataset import collate_fn, llavadataset\n",
    "\n",
    "# Proxy setup, if necessary\n",
    "try:\n",
    "    os.environ['HTTP_PROXY'] = 'http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY'] = 'http://185.46.212.90:80'\n",
    "    print(\"Proxy exported\")\n",
    "except Exception as e:\n",
    "    print(\"Could not set proxy:\", e)\n",
    "\n",
    "# Ensure CUDA is available, otherwise fall back to CPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using CUDA: {torch.cuda.device_count()} GPUs available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"coco_dataset_pickle\", \"rb\") as fp:\n",
    "    coco_unpickle = pickle.load(fp)\n",
    "\n",
    "# Tokenizer and model setup\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name = \"microsoft/phi-2\"\n",
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "tokenizer = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True, use_cache=True)\n",
    "tokenizer.save_pretrained(\"saved_tokenizer\")\n",
    "\n",
    "# Model initialization and DataParallel wrapping\n",
    "MModalGPT = CLIPPhi2Model()\n",
    "if torch.cuda.is_available():\n",
    "    MModalGPT = torch.nn.DataParallel(MModalGPT).to(device)\n",
    "\n",
    "# Data loaders setup\n",
    "train_dataloader = DataLoader(\n",
    "    llavadataset(coco_unpickle, phi_model_name, clip_model_name, 'train', tokenizer),\n",
    "    collate_fn=collate_fn, batch_size=train_batch_size, num_workers=20, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    llavadataset(coco_unpickle, phi_model_name, clip_model_name, 'val', tokenizer),\n",
    "    collate_fn=collate_fn, batch_size=val_batch_size, num_workers=20, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-6)\n",
    "\n",
    "# Set float32_matmul_precision to 'medium'\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Train the model\n",
    "train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps=100000, model_save_step=1000, model_val_step=1000, log_step=100, max_token_filter=35, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dc1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is for running in local ###\n",
    "import os\n",
    "try:\n",
    "    os.environ['HTTP_PROXY']='http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY']='http://185.46.212.90:80'\n",
    "    print (\"proxy_exported\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from step1_network import CLIPPhi2Model, train_model\n",
    "from step1_dataset import collate_fn, llavadataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability and fallback to CPU if not available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8103b76-5435-4f19-8d1b-bee880404519",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coco_dataset_pickle\", \"rb\") as fp:   # Unpickling\n",
    "    coco_unpickle = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3980de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b01a16-1989-460e-bce4-be20b1cefd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_name  = \"openai/clip-vit-base-patch32\"\n",
    "phi_model_name   = \"microsoft/phi-2\"\n",
    "train_batch_size = 2 #2\n",
    "val_batch_size   = 4 #4\n",
    "tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"saved_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "MModalGPT        = CLIPPhi2Model().to(device)\n",
    "max_steps        = 100 #100000\n",
    "model_save_step  = 10 #1000\n",
    "model_val_step   = 2 #1000\n",
    "log_step         = 2 #1000\n",
    "max_token_filter = 35 #35 # memory management restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056e4a3-a835-4dd9-8bb7-cb84e462ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_dataloader = DataLoader(llavadataset(coco_unpickle[0:100], phi_model_name,clip_model_name,'train',tokenizer),\n",
    "                  collate_fn=collate_fn, batch_size=train_batch_size, num_workers = 2, shuffle=True, pin_memory=True)\n",
    "val_dataloader   = DataLoader(llavadataset(coco_unpickle[0:100], phi_model_name,clip_model_name,'val',tokenizer),\n",
    "                  collate_fn=collate_fn, batch_size=val_batch_size, num_workers = 2, shuffle=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1db0d-7e18-46c7-9a70-748ad26cc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f16870-145b-4752-ad0e-576152d256bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
